{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "# Train a deep learning model\n",
        "In this notebook you will train a deep learning model to classify the descriptions of parts components as compliant or non-compliant. \n",
        "\n",
        "Each document in the supplied training data set is a short text description of the component as documented by an authorized technician. \n",
        "The contents include:\n",
        "- Manufacture year of the component (e.g. 1985, 2010)\n",
        "- Condition of the component (poor, fair, good, new)\n",
        "- Materials used in the component (plastic, carbon fiber, steel, iron)\n",
        "\n",
        "The compliance regulations dictate:\n",
        "*Any component manufactured before 1995 or in fair or poor condition or made with plastic or iron is out of compliance.*\n",
        "\n",
        "For example:\n",
        "* Manufactured in 1985 made of steel in fair condition -> **Non-compliant**\n",
        "* Good condition carbon fiber component manufactured in 2010 -> **Compliant**\n",
        "* Steel component manufactured in 1995 in fair condition -> **Non-Compliant**\n",
        "\n",
        "The labels present in this data are 0 for compliant, 1 for non-compliant.\n",
        "\n",
        "The challenge with classifying text data is that deep learning models only undertand vectors (e.g., arrays of numbers) and not text. To encode the car component descriptions as vectors, we use an algorithm from Stanford called [GloVe (Global Vectors for Word Representation)](https://nlp.stanford.edu/projects/glove/). GloVe provides us pre-trained vectors that we can use to convert a string of text into a vector."
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import keras\n",
        "from keras import models \n",
        "from keras import layers\n",
        "from keras import optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "project_folder = './dl'\n",
        "deployment_folder = './deploy'\n",
        "\n",
        "# create project folder\n",
        "if not os.path.exists(project_folder):\n",
        "    os.makedirs(project_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# this is the URL to the CSV file containing the GloVe vectors\n",
        "glove_url = ('https://davewdemoblobs.blob.core.windows.net/mlops/glove.6B.100d.txt')\n",
        "# this is the URL to the CSV file containing the care component descriptions\n",
        "data_url = ('https://davewdemoblobs.blob.core.windows.net/mlops/components.csv')\n",
        "\n",
        "def download_glove():\n",
        "    print(\"Downloading GloVe embeddings...\")\n",
        "    import urllib.request\n",
        "    urllib.request.urlretrieve(glove_url, 'glove.6B.100d.txt')\n",
        "    print(\"Download complete.\")\n",
        "\n",
        "download_glove()\n",
        "\n",
        "\n",
        "# Load the components labeled data\n",
        "print(\"Loading components data...\")\n",
        "components_df = pd.read_csv(data_url)\n",
        "components = components_df[\"text\"].tolist()\n",
        "labels = components_df[\"label\"].tolist()\n",
        "print(\"Loading components data completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# split data 60% for trianing, 20% for validation, 20% for test\n",
        "print(\"Splitting data...\")\n",
        "train, validate, test = np.split(components_df.sample(frac=1), [int(.6*len(components_df)), int(.8*len(components_df))])\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "print(validate.shape)\n",
        "\n",
        "# use the Tokenizer from Keras to \"learn\" a vocabulary from the entire car components text\n",
        "print(\"Tokenizing data...\")\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100                                           \n",
        "training_samples = 90000                                 \n",
        "validation_samples = 5000    \n",
        "max_words = 10000      \n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(components)\n",
        "sequences = tokenizer.texts_to_sequences(components)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "indices = np.arange(data.shape[0])                     \n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]\n",
        "\n",
        "x_test = data[training_samples + validation_samples:]\n",
        "y_test = labels[training_samples + validation_samples:]\n",
        "print(\"Tokenizing data complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# apply the vectors provided by GloVe to create a word embedding matrix\n",
        "print(\"Applying GloVe vectors...\")\n",
        "glove_dir =  './'\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector    \n",
        "print(\"Applying GloVe vectors compelted.\")\n",
        "\n",
        "# use Keras to define the structure of the deep neural network   \n",
        "print(\"Creating model structure...\")\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "# fix the weights for the first layer to those provided by the embedding matrix\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "print(\"Creating model structure completed.\")\n",
        "\n",
        "opt = optimizers.RMSprop(lr=0.1)\n",
        "\n",
        "print(\"Training model...\")\n",
        "model.compile(optimizer=opt,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=1, \n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))\n",
        "print(\"Training model completed.\")\n",
        "\n",
        "print(\"Saving model files...\")\n",
        "# create a ./outputs/model folder in the compute target\n",
        "# files saved in the \"./outputs\" folder are automatically uploaded into run history\n",
        "os.makedirs('./outputs/model', exist_ok=True)\n",
        "# save model\n",
        "model.save('./outputs/model/model.h5')\n",
        "print(\"model saved in ./outputs/model folder\")\n",
        "print(\"Saving model files completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "## Restore the model from model.h5 file"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model = load_model('./model/model.h5')\n",
        "print(\"Model loaded from disk.\")\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "You can also evaluate how accurately the model performs against data it has not seen. Run the following cell to load the test data that was not used in either training or evaluating the model. "
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Load the components labeled data\n",
        "components_df = pd.read_csv(data_url)\n",
        "components = components_df[\"text\"].tolist()\n",
        "labels = components_df[\"label\"].tolist()\n",
        "\n",
        "maxlen = 100                                           \n",
        "training_samples = 90000                                 \n",
        "validation_samples = 5000    \n",
        "max_words = 10000      \n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(components)\n",
        "sequences = tokenizer.texts_to_sequences(components)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "indices = np.arange(data.shape[0])                     \n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_test = data[training_samples + validation_samples:]\n",
        "y_test = labels[training_samples + validation_samples:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Run the following cell to see the accuracy on the test set (it is the second number in the array displayed, on a scale from 0 to 1)."
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "print('Model evaluation will print the following metrics: ', model.metrics_names)\n",
        "evaluation_metrics = model.evaluate(x_test, y_test)\n",
        "print(evaluation_metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "name": "Deep Learning",
    "notebookId": 2340934485665719
  },
  "nbformat": 4,
  "nbformat_minor": 1
}