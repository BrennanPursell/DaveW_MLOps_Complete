{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Automated Machine Learning \n",
        "**Continuous retraining using Pipelines**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "In this example we use AutoML and Pipelines to enable contious retraining of a model based on updates to the training dataset. We will create two pipelines:\n",
        "* one to demonstrate a training dataset that gets updated over time. \n",
        "* The second pipeline utilizes pipeline `Schedule` to trigger continuous retraining. \n",
        "\n",
        "In this notebook you will learn how to:\n",
        "* Create an Experiment in an existing Workspace.\n",
        "* Configure AutoML using AutoMLConfig.\n",
        "* Create data ingestion pipeline to update a dataset\n",
        "* Create training pipeline to prepare data, run AutoML, register the model and setup pipeline triggers.\n",
        "\n",
        "## Setup\n",
        "As part of the setup you have already created an Azure ML `Workspace` object. For AutoML you will need to create an `Experiment` object, which is a named object in a `Workspace` used to run experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "\n",
        "import azureml.core\n",
        "from azureml.core.experiment import Experiment\n",
        "from azureml.core.workspace import Workspace\n",
        "from azureml.train.automl import AutoMLConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This sample notebook may use features that are not available in previous versions of the Azure ML SDK.\n",
        "If needed run this:  `!pip install --upgrade --upgrade-strategy eager azureml-sdk`.  If you are running an AMLS Compute instance it's probably better to just rebuild the compute note. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"This notebook was created using version 1.18.0 of the Azure ML SDK\")\n",
        "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accessing the Azure ML workspace requires authentication with Azure.\n",
        "\n",
        "The default authentication is interactive authentication using the default tenant. Executing the ws = Workspace.from_config() line in the cell below will prompt for authentication the first time that it is run.\n",
        "\n",
        "If you have multiple Azure tenants, you can specify the tenant by replacing the ws = Workspace.from_config() line in the cell below with the following:\n",
        "```\n",
        "from azureml.core.authentication import InteractiveLoginAuthentication\n",
        "auth = InteractiveLoginAuthentication(tenant_id = 'mytenantid')\n",
        "ws = Workspace.from_config(auth = auth)\n",
        "```\n",
        "If you need to run in an environment where interactive login is not possible, you can use Service Principal authentication by replacing the ws = Workspace.from_config() line in the cell below with the following:\n",
        "```\n",
        "from azureml.core.authentication import ServicePrincipalAuthentication\n",
        "auth = auth = ServicePrincipalAuthentication('mytenantid', 'myappid', 'mypassword')\n",
        "ws = Workspace.from_config(auth = auth)\n",
        "```\n",
        "For more details, see aka.ms/aml-notebook-auth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ws = Workspace.from_config()\n",
        "output = {}\n",
        "output['Subscription ID'] = ws.subscription_id\n",
        "output['Workspace'] = ws.name\n",
        "output['Resource Group'] = ws.resource_group\n",
        "output['Location'] = ws.location\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dstor = ws.get_default_datastore()\n",
        "\n",
        "# Choose a name for the run history container in the workspace.\n",
        "experiment_name = 'ar-factoring-2class-autoretrain'\n",
        "experiment = Experiment(ws, experiment_name)\n",
        "\n",
        "output['Run History Name'] = experiment_name\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "outputDf = pd.DataFrame(data = output, index = [''])\n",
        "outputDf.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you look in your workspace the experiment is not yet created.  \n",
        "\n",
        "## Compute \n",
        "\n",
        "#### Create or Attach existing AmlCompute\n",
        "\n",
        "You will need to create a compute target for your AutoML run, or use existing.  \n",
        "#### Creation of AmlCompute takes approximately 5 minutes. \n",
        "If the AmlCompute with that name is already in your workspace this code will skip the creation process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# Choose a name for your CPU cluster, or use the existing compute cluster\n",
        "amlcompute_cluster_name = \"automl\"\n",
        "\n",
        "# Verify that cluster does not exist already\n",
        "try:\n",
        "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
        "    print('Found existing cluster, use it.')\n",
        "except ComputeTargetException:\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
        "                                                           max_nodes=4)\n",
        "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
        "\n",
        "compute_target.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core.runconfig import CondaDependencies, RunConfiguration\n",
        "\n",
        "# create a new RunConfig object\n",
        "conda_run_config = RunConfiguration(framework=\"python\")\n",
        "\n",
        "# Set compute target to AmlCompute\n",
        "conda_run_config.target = compute_target\n",
        "\n",
        "conda_run_config.environment.docker.enabled = True\n",
        "\n",
        "cd = CondaDependencies.create(pip_packages=['azureml-sdk[automl]', 'applicationinsights', 'azureml-opendatasets', 'azureml-defaults'], \n",
        "                              conda_packages=['numpy==1.16.2'], \n",
        "                              pin_sdk_version=False)\n",
        "conda_run_config.environment.python.conda_dependencies = cd\n",
        "\n",
        "print('run config is ready')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Ingestion Pipeline \n",
        "For this lab, we are simply going to pull a copy of the existing data directly from our github repo, overriding the existing data.  In the real world we would pull the latest data into the registered dataset.  Simply making a copy of the data is sufficient because the copy will set the flags for the last time the data was updated.  We can use that information later to determine if we want to start a retraining event.  \n",
        "\n",
        "In the next cell we have a little python program that simply copies the data to the existing dataset/datastore.  When we build a AMLS pipeline we need to use a `python script` so this cell actually builds a python script (the first line does this) for us.  Change any variables you need and run this cell.  You should see the `upload_latest_data.py` is updated.  \n",
        "\n",
        "Let's create a subfolder just for this script file.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "project_folder = './ar-pipeline'\n",
        "\n",
        "# create project folder\n",
        "if not os.path.exists(project_folder):\n",
        "    os.makedirs(project_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $project_folder/upload_latest_data.py\n",
        "\n",
        "# vars to change\n",
        "web_paths = ['https://raw.githubusercontent.com/davew-msft/MLOps-E2E/master/Lab43/WA_Fn-UseC_-Accounts-Receivable.csv']\n",
        "# the name of your dataset in AMLS\n",
        "ds_name = 'ar-factoring-2class'\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import pandas as pd\n",
        "import traceback\n",
        "from azureml.core import Dataset\n",
        "from azureml.core.run import Run, _OfflineRun\n",
        "from azureml.core import Workspace\n",
        "from azureml.opendatasets import NoaaIsdWeather\n",
        "\n",
        "run = Run.get_context()\n",
        "ws = None\n",
        "if type(run) == _OfflineRun:\n",
        "    ws = Workspace.from_config()\n",
        "else:\n",
        "    ws = run.experiment.workspace\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(\"split\")\n",
        "parser.add_argument(\"--descr\", help=\"the descr has to be updated or the dataset will not be re-downloaded\")\n",
        "args = parser.parse_args()\n",
        "\n",
        "print(\"Argument 1(descr): %s\" % args.descr)\n",
        "descr = args.descr or \"default descr\"\n",
        "\n",
        "ar_ds = Dataset.Tabular.from_delimited_files(path=web_paths)\n",
        "# create a new version of our dataset\n",
        "ar_ds = ar_ds.register(workspace = ws,\n",
        "                                 name = ds_name,\n",
        "                                 description = descr,\n",
        "                                 create_new_version = True)\n"
      ]
    },
    {
      "source": [
        "Let's see where the file was written.  We will use this file as the code for the AMLS pipeline next.  Also, let's test that the .py file actually works!"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!ls ./ar-pipeline/ -alF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run ./ar-pipeline/upload_latest_data.py  --descr \"Get Latest1\""
      ]
    },
    {
      "source": [
        "If it worked we should see a new version of our dataset in the AMLS dataset UI.  Go check this now."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Upload Data Step\n",
        "The data ingestion pipeline has a single step with a script to get the latest data and upload it to our dataset/datastore as a new version. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.pipeline.core import Pipeline, PipelineParameter\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "from datetime import date\n",
        "\n",
        "ds_descr = PipelineParameter(name=\"descr\", default_value=\"This is the default descr\")\n",
        "upload_data_step = PythonScriptStep(script_name=\"upload_latest_data.py\", \n",
        "                                         allow_reuse=False,\n",
        "                                         name=\"upload_latest_data\",\n",
        "                                         arguments=[\"--descr\", ds_descr],\n",
        "                                         compute_target=compute_target, \n",
        "                                         runconfig=conda_run_config,\n",
        "                                         source_directory=project_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you look in AMLS the pipeline nor the experiment is created yet. We need to submit it first.\n",
        "\n",
        "### Submit Pipeline Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# this changes the description which is enough to cause a reload of the dataset\n",
        "latest = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\")\n",
        "\n",
        "data_pipeline = Pipeline(\n",
        "    description=\"pipeline to upload latest AR data\",\n",
        "    workspace=ws,    \n",
        "    steps=[upload_data_step])\n",
        "data_pipeline_run = experiment.submit(data_pipeline, pipeline_parameters={\"descr\":latest})"
      ]
    },
    {
      "source": [
        "Now you can either monitor this from the AMLS portal or do it with the next cell"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "outputPrepend"
        ]
      },
      "outputs": [],
      "source": [
        "data_pipeline_run.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Pipeline\n",
        "### Prepare Training Data Step\n",
        "\n",
        "Script to check if new data is available since the model was last trained. If no new data is available, we cancel the remaining pipeline steps. We need to set allow_reuse flag to False to allow the pipeline to run even when inputs don't change. We also need the name of the model to check the time the model was last trained.\n",
        "\n",
        "First, like above, we need to create a `check_data.py` script that our Pipeline will use.  We'll build that using the tricks above, in our project folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $project_folder/check_data.py\n",
        "\n",
        "# vars to change\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import azureml.core\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import pytz\n",
        "from azureml.core import Dataset, Model\n",
        "from azureml.core.run import Run, _OfflineRun\n",
        "from azureml.core import Workspace\n",
        "\n",
        "run = Run.get_context()\n",
        "ws = None\n",
        "if type(run) == _OfflineRun:\n",
        "    ws = Workspace.from_config()\n",
        "else:\n",
        "    ws = run.experiment.workspace\n",
        "\n",
        "print(\"Check for new data.\")\n",
        "\n",
        "parser = argparse.ArgumentParser(\"split\")\n",
        "parser.add_argument(\"--ds_name\", help=\"input dataset name\")\n",
        "parser.add_argument(\"--model_name\", help=\"name of the deployed model\")\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "print(\"Argument 1(ds_name): %s\" % args.ds_name)\n",
        "print(\"Argument 2(model_name): %s\" % args.model_name)\n",
        "\n",
        "# Get the latest registered model\n",
        "try:\n",
        "    model = Model(ws, args.model_name)\n",
        "    last_train_time = model.created_time\n",
        "    print(\"Model was last trained on {0}.\".format(last_train_time))\n",
        "except Exception as e:\n",
        "    print(\"Could not get last model train time.\")\n",
        "    last_train_time = datetime.min.replace(tzinfo=pytz.UTC)\n",
        "\n",
        "try: \n",
        "    train_ds = Dataset.get_by_name(ws, args.ds_name)\n",
        "    format = \"%Y/%m/%d %H:%M:%S\"\n",
        "    dataset_changed_time = datetime.strptime(train_ds.description,format)\n",
        "    print (\"Data was last updated on {0}.\".format(dataset_changed_time))\n",
        "    if not dataset_changed_time > last_train_time:\n",
        "        print(\"Cancelling run since there is no new data.\")\n",
        "        run.parent.cancel()\n",
        "    else:\n",
        "        # New data is available since the model was last trained\n",
        "        print(\"Dataset was last updated on {0}. Retraining...\".format(dataset_changed_time))\n",
        "except Exception as e:\n",
        "    print(\"Date not in the format we were expecting, do a re-train anyway.\")\n",
        "\n"
      ]
    },
    {
      "source": [
        "Now, let's test the script above, just to make sure it works."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# vars to change\n",
        "# since we used automl the name is probably a little goofy.  Don't include the :1 which is the version indicator\n",
        "registered_model_name = \"AutoMLb9be0a22f28\"\n",
        "dataset_name = \"ar-factoring-2class\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run $project_folder/check_data.py  --ds_name $dataset_name --model_name $registered_model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.pipeline.core import PipelineData\n",
        "\n",
        "# The model name with which to register the trained model in the workspace.\n",
        "model_name = PipelineParameter(\"model_name\", default_value=registered_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_prep_step = PythonScriptStep(script_name=\"check_data.py\", \n",
        "                                         allow_reuse=False,\n",
        "                                         name=\"check_data\",\n",
        "                                         arguments=[\"--ds_name\", dataset_name,\n",
        "                                                    \"--model_name\", registered_model_name],\n",
        "                                         compute_target=compute_target, \n",
        "                                         runconfig=conda_run_config,\n",
        "                                         source_directory=project_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core import Dataset\n",
        "train_ds = Dataset.get_by_name(ws, dataset_name)\n",
        "target_column_name=\"LatePayment\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create an automl step for the pipeline\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from azureml.pipeline.steps import AutoMLStep\n",
        "\n",
        "automl_settings = {\n",
        "    \"iteration_timeout_minutes\": 3,\n",
        "    \"experiment_timeout_hours\": 0.15,\n",
        "    \"n_cross_validations\": 3,\n",
        "    \"primary_metric\": 'accuracy',\n",
        "    \"max_concurrent_iterations\": 3,\n",
        "    \"max_cores_per_iteration\": -1,\n",
        "    \"verbosity\": logging.INFO,\n",
        "    \"enable_early_stopping\": True\n",
        "}\n",
        "\n",
        "automl_config = AutoMLConfig(task = 'classification',\n",
        "                             debug_log = 'automl_errors.log',\n",
        "                             path = \".\",\n",
        "                             compute_target=compute_target,\n",
        "                             training_data = train_ds,\n",
        "                             label_column_name = target_column_name,\n",
        "                             **automl_settings\n",
        "                            )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Register Model Step\n",
        "Script to register the model to the workspace. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $project_folder/check_data.py\n",
        "# we need to build a py script to register our model in the pipeline\n",
        "from azureml.core.model import Model, Dataset\n",
        "from azureml.core.run import Run, _OfflineRun\n",
        "from azureml.core import Workspace\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--model_name\")\n",
        "parser.add_argument(\"--model_path\")\n",
        "parser.add_argument(\"--ds_name\")\n",
        "args = parser.parse_args()\n",
        "\n",
        "print(\"Argument 1(model_name): %s\" % args.model_name)\n",
        "print(\"Argument 2(model_path): %s\" % args.model_path)\n",
        "print(\"Argument 3(ds_name): %s\" % args.ds_name)\n",
        "\n",
        "run = Run.get_context()\n",
        "ws = None\n",
        "if type(run) == _OfflineRun:\n",
        "    ws = Workspace.from_config()\n",
        "else:\n",
        "    ws = run.experiment.workspace\n",
        "\n",
        "train_ds = Dataset.get_by_name(ws, args.ds_name)\n",
        "datasets = [(Dataset.Scenario.TRAINING, train_ds)]\n",
        "\n",
        "# Register model with training dataset\n",
        "\n",
        "model = Model.register(workspace=ws,\n",
        "                       model_path=args.model_path,\n",
        "                       model_name=args.model_name,\n",
        "                       datasets=datasets)\n",
        "\n",
        "print(\"Registered version {0} of model {1}\".format(model.version, model.name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "register_model_step = PythonScriptStep(script_name=\"register_model.py\",\n",
        "                                       name=\"register_model\",\n",
        "                                       allow_reuse=False,\n",
        "                                       arguments=[\"--model_name\", model_name, \"--model_path\", model_data, \"--ds_name\", ds_name],\n",
        "                                       inputs=[model_data],\n",
        "                                       compute_target=compute_target,\n",
        "                                       runconfig=conda_run_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Submit Pipeline Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_pipeline = Pipeline(\n",
        "    description=\"training_pipeline\",\n",
        "    workspace=ws,    \n",
        "    steps=[data_prep_step, automl_step, register_model_step])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_pipeline_run = experiment.submit(training_pipeline, pipeline_parameters={\n",
        "        \"ds_name\": dataset, \"model_name\": registered_model_name})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_pipeline_run.wait_for_completion(show_output=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Publish Retraining Pipeline and Schedule\n",
        "Once we are happy with the pipeline, we can publish the training pipeline to the workspace and create a schedule to trigger on blob change. The schedule polls the blob store where the data is being uploaded and runs the retraining pipeline if there is a data change. A new version of the model will be registered to the workspace once the run is complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_name = \"Retraining-Pipeline-AR-Factoring\"\n",
        "\n",
        "published_pipeline = training_pipeline.publish(\n",
        "    name=pipeline_name, \n",
        "    description=\"Pipeline that retrains AutoML model\")\n",
        "\n",
        "published_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.pipeline.core import Schedule\n",
        "schedule = Schedule.create(workspace=ws, name=\"RetrainingSchedule\",\n",
        "                           pipeline_parameters={\"ds_name\": dataset, \"model_name\": registered_model_name},\n",
        "                           pipeline_id=published_pipeline.id, \n",
        "                           experiment_name=experiment_name, \n",
        "                           datastore=dstor,\n",
        "                           wait_for_provisioning=True,\n",
        "                           polling_interval=1440)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Retraining\n",
        "Here we setup the data ingestion pipeline to run on a schedule, to verify that the retraining pipeline runs as expected. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_name = \"DataIngestion-Pipeline-AR Factoring Dataset\"\n",
        "\n",
        "published_pipeline = training_pipeline.publish(\n",
        "    name=pipeline_name, \n",
        "    description=\"Pipeline that updates AR Factoring Dataset\")\n",
        "\n",
        "published_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.pipeline.core import Schedule\n",
        "schedule = Schedule.create(workspace=ws, name=\"RetrainingSchedule-DataIngestion\",\n",
        "                           pipeline_parameters={\"ds_name\":dataset},\n",
        "                           pipeline_id=published_pipeline.id, \n",
        "                           experiment_name=experiment_name, \n",
        "                           datastore=dstor,\n",
        "                           wait_for_provisioning=True,\n",
        "                           polling_interval=1440)"
      ]
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "vivijay"
      }
    ],
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}