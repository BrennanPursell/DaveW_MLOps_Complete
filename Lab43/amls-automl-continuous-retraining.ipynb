{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Automated Machine Learning \n",
        "**Continuous retraining using Pipelines**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "In this example we use AutoML and Pipelines to enable contious retraining of a model based on updates to the training dataset. We will create two pipelines:\n",
        "* one to demonstrate a training dataset that gets updated over time. \n",
        "* The second pipeline utilizes pipeline `Schedule` to trigger continuous retraining. \n",
        "\n",
        "In this notebook you will learn how to:\n",
        "* Create an Experiment in an existing Workspace.\n",
        "* Configure AutoML using AutoMLConfig.\n",
        "* Create data ingestion pipeline to update a dataset\n",
        "* Create training pipeline to prepare data, run AutoML, register the model and setup pipeline triggers.\n",
        "\n",
        "## Setup\n",
        "As part of the setup you have already created an Azure ML `Workspace` object. For AutoML you will need to create an `Experiment` object, which is a named object in a `Workspace` used to run experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "\n",
        "import azureml.core\n",
        "from azureml.core.experiment import Experiment\n",
        "from azureml.core.workspace import Workspace\n",
        "from azureml.train.automl import AutoMLConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This sample notebook may use features that are not available in previous versions of the Azure ML SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This notebook was created using version 1.18.0 of the Azure ML SDK\nYou are currently using version 1.11.0 of the Azure ML SDK\n"
          ]
        }
      ],
      "source": [
        "print(\"This notebook was created using version 1.18.0 of the Azure ML SDK\")\n",
        "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accessing the Azure ML workspace requires authentication with Azure.\n",
        "\n",
        "The default authentication is interactive authentication using the default tenant. Executing the ws = Workspace.from_config() line in the cell below will prompt for authentication the first time that it is run.\n",
        "\n",
        "If you have multiple Azure tenants, you can specify the tenant by replacing the ws = Workspace.from_config() line in the cell below with the following:\n",
        "```\n",
        "from azureml.core.authentication import InteractiveLoginAuthentication\n",
        "auth = InteractiveLoginAuthentication(tenant_id = 'mytenantid')\n",
        "ws = Workspace.from_config(auth = auth)\n",
        "```\n",
        "If you need to run in an environment where interactive login is not possible, you can use Service Principal authentication by replacing the ws = Workspace.from_config() line in the cell below with the following:\n",
        "```\n",
        "from azureml.core.authentication import ServicePrincipalAuthentication\n",
        "auth = auth = ServicePrincipalAuthentication('mytenantid', 'myappid', 'mypassword')\n",
        "ws = Workspace.from_config(auth = auth)\n",
        "```\n",
        "For more details, see aka.ms/aml-notebook-auth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Subscription ID': '52061d21-01dd-4f9e-aca9-60fff4d67ee2', 'Workspace': 'mlops', 'Resource Group': 'MLOpsWorkshop', 'Location': 'eastus'}\n"
          ]
        }
      ],
      "source": [
        "ws = Workspace.from_config()\n",
        "output = {}\n",
        "output['Subscription ID'] = ws.subscription_id\n",
        "output['Workspace'] = ws.name\n",
        "output['Resource Group'] = ws.resource_group\n",
        "output['Location'] = ws.location\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                      \n",
              "Subscription ID   52061d21-01dd-4f9e-aca9-60fff4d67ee2\n",
              "Workspace         mlops                               \n",
              "Resource Group    MLOpsWorkshop                       \n",
              "Location          eastus                              \n",
              "Run History Name  ar-factoring-2class-autoretrain     "
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Subscription ID</th>\n      <td>52061d21-01dd-4f9e-aca9-60fff4d67ee2</td>\n    </tr>\n    <tr>\n      <th>Workspace</th>\n      <td>mlops</td>\n    </tr>\n    <tr>\n      <th>Resource Group</th>\n      <td>MLOpsWorkshop</td>\n    </tr>\n    <tr>\n      <th>Location</th>\n      <td>eastus</td>\n    </tr>\n    <tr>\n      <th>Run History Name</th>\n      <td>ar-factoring-2class-autoretrain</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "dstor = ws.get_default_datastore()\n",
        "\n",
        "# Choose a name for the run history container in the workspace.\n",
        "experiment_name = 'ar-factoring-2class-autoretrain'\n",
        "experiment = Experiment(ws, experiment_name)\n",
        "\n",
        "output['Run History Name'] = experiment_name\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "outputDf = pd.DataFrame(data = output, index = [''])\n",
        "outputDf.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you look in your workspace the experiment is not yet created.  \n",
        "\n",
        "## Compute \n",
        "\n",
        "#### Create or Attach existing AmlCompute\n",
        "\n",
        "You will need to create a compute target for your AutoML run, or use existing.  \n",
        "#### Creation of AmlCompute takes approximately 5 minutes. \n",
        "If the AmlCompute with that name is already in your workspace this code will skip the creation process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing cluster, use it.\n",
            "Succeeded\n",
            "AmlCompute wait for completion finished\n",
            "\n",
            "Minimum number of nodes requested have been provisioned\n"
          ]
        }
      ],
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# Choose a name for your CPU cluster, or use the existing compute cluster\n",
        "amlcompute_cluster_name = \"automl\"\n",
        "\n",
        "# Verify that cluster does not exist already\n",
        "try:\n",
        "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
        "    print('Found existing cluster, use it.')\n",
        "except ComputeTargetException:\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
        "                                                           max_nodes=4)\n",
        "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
        "\n",
        "compute_target.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run config is ready\n"
          ]
        }
      ],
      "source": [
        "from azureml.core.runconfig import CondaDependencies, RunConfiguration\n",
        "\n",
        "# create a new RunConfig object\n",
        "conda_run_config = RunConfiguration(framework=\"python\")\n",
        "\n",
        "# Set compute target to AmlCompute\n",
        "conda_run_config.target = compute_target\n",
        "\n",
        "conda_run_config.environment.docker.enabled = True\n",
        "\n",
        "cd = CondaDependencies.create(pip_packages=['azureml-sdk[automl]', 'applicationinsights', 'azureml-opendatasets', 'azureml-defaults'], \n",
        "                              conda_packages=['numpy==1.16.2'], \n",
        "                              pin_sdk_version=False)\n",
        "conda_run_config.environment.python.conda_dependencies = cd\n",
        "\n",
        "print('run config is ready')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Ingestion Pipeline \n",
        "For this lab, we are simply going to pull a copy of the existing data directly from our github repo, overriding the existing data.  In the real world we would pull the latest data into the registered dataset.  Simply making a copy of the data is sufficient because the copy will set the flags for the last time the data was updated.  We can use that information later to determine if we want to start a retraining event.  \n",
        "\n",
        "In the next cell we have a little python program that simply copies the data to the existing dataset/datastore.  When we build a AMLS pipeline we need to use a `python script` so this cell actually builds a python script (the first line does this) for us.  Change any variables you need and run this cell.  You should see the `upload_latest_data.py` is updated.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile upload_latest_data.py\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import pandas as pd\n",
        "import traceback\n",
        "from azureml.core import Dataset\n",
        "from azureml.core.run import Run, _OfflineRun\n",
        "from azureml.core import Workspace\n",
        "from azureml.opendatasets import NoaaIsdWeather\n",
        "\n",
        "run = Run.get_context()\n",
        "ws = None\n",
        "if type(run) == _OfflineRun:\n",
        "    ws = Workspace.from_config()\n",
        "else:\n",
        "    ws = run.experiment.workspace\n",
        "\n",
        "usaf_list = ['725724', '722149', '723090', '722159', '723910', '720279',\n",
        "             '725513', '725254', '726430', '720381', '723074', '726682',\n",
        "             '725486', '727883', '723177', '722075', '723086', '724053',\n",
        "             '725070', '722073', '726060', '725224', '725260', '724520',\n",
        "             '720305', '724020', '726510', '725126', '722523', '703333',\n",
        "             '722249', '722728', '725483', '722972', '724975', '742079',\n",
        "             '727468', '722193', '725624', '722030', '726380', '720309',\n",
        "             '722071', '720326', '725415', '724504', '725665', '725424',\n",
        "             '725066']\n",
        "\n",
        "\n",
        "def get_noaa_data(start_time, end_time):\n",
        "    columns = ['usaf', 'wban', 'datetime', 'latitude', 'longitude', 'elevation',\n",
        "               'windAngle', 'windSpeed', 'temperature', 'stationName', 'p_k']\n",
        "    isd = NoaaIsdWeather(start_time, end_time, cols=columns)\n",
        "    noaa_df = isd.to_pandas_dataframe()\n",
        "    df_filtered = noaa_df[noaa_df[\"usaf\"].isin(usaf_list)]\n",
        "    df_filtered.reset_index(drop=True)\n",
        "    print(\"Received {0} rows of training data between {1} and {2}\".format(\n",
        "        df_filtered.shape[0], start_time, end_time))\n",
        "    return df_filtered\n",
        "\n",
        "\n",
        "print(\"Check for new data and prepare the data\")\n",
        "\n",
        "parser = argparse.ArgumentParser(\"split\")\n",
        "parser.add_argument(\"--ds_name\", help=\"name of the Dataset to update\")\n",
        "args = parser.parse_args()\n",
        "\n",
        "print(\"Argument 1(ds_name): %s\" % args.ds_name)\n",
        "\n",
        "dstor = ws.get_default_datastore()\n",
        "register_dataset = False\n",
        "try:\n",
        "    ds = Dataset.get_by_name(ws, args.ds_name)\n",
        "    end_time_last_slice = ds.data_changed_time.replace(tzinfo=None)\n",
        "    print(\"Dataset {0} last updated on {1}\".format(args.ds_name,\n",
        "                                                   end_time_last_slice))\n",
        "except Exception as e:\n",
        "    print(traceback.format_exc())\n",
        "    print(\"Dataset with name {0} not found, registering new dataset.\".format(args.ds_name))\n",
        "    register_dataset = True\n",
        "    end_time_last_slice = datetime.today() - relativedelta(weeks=2)\n",
        "\n",
        "end_time = datetime.utcnow()\n",
        "train_df = get_noaa_data(end_time_last_slice, end_time)\n",
        "\n",
        "if train_df.size > 0:\n",
        "    print(\"Received {0} rows of new data after {0}.\".format(\n",
        "        train_df.shape[0], end_time_last_slice))\n",
        "    folder_name = \"{}/{:04d}/{:02d}/{:02d}/{:02d}/{:02d}/{:02d}\".format(args.ds_name, end_time.year,\n",
        "                                                                        end_time.month, end_time.day,\n",
        "                                                                        end_time.hour, end_time.minute,\n",
        "                                                                        end_time.second)\n",
        "    file_path = \"{0}/data.csv\".format(folder_name)\n",
        "\n",
        "    # Add a new partition to the registered dataset\n",
        "    os.makedirs(folder_name, exist_ok=True)\n",
        "    train_df.to_csv(file_path, index=False)\n",
        "\n",
        "    dstor.upload_files(files=[file_path],\n",
        "                       target_path=folder_name,\n",
        "                       overwrite=True,\n",
        "                       show_progress=True)\n",
        "else:\n",
        "    print(\"No new data since {0}.\".format(end_time_last_slice))\n",
        "\n",
        "if register_dataset:\n",
        "    ds = Dataset.Tabular.from_delimited_files(dstor.path(\"{}/**/*.csv\".format(\n",
        "        args.ds_name)), partition_format='/{partition_date:yyyy/MM/dd/HH/mm/ss}/data.csv')\n",
        "    ds.register(ws, name=args.ds_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The name and target column of the Dataset to create \n",
        "dataset = \"NOAA-Weather-DS4\"\n",
        "target_column_name = \"temperature\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Upload Data Step\n",
        "The data ingestion pipeline has a single step with a script to query the latest weather data and upload it to the blob store. During the first run, the script will create and register a time-series based `TabularDataset` with the past one week of weather data. For each subsequent run, the script will create a partition in the blob store by querying NOAA for new weather data since the last modified time of the dataset (`dataset.data_changed_time`) and creating a data.csv file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.pipeline.core import Pipeline, PipelineParameter\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "\n",
        "ds_name = PipelineParameter(name=\"ds_name\", default_value=dataset)\n",
        "upload_data_step = PythonScriptStep(script_name=\"upload_weather_data.py\", \n",
        "                                         allow_reuse=False,\n",
        "                                         name=\"upload_weather_data\",\n",
        "                                         arguments=[\"--ds_name\", ds_name],\n",
        "                                         compute_target=compute_target, \n",
        "                                         runconfig=conda_run_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Submit Pipeline Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_pipeline = Pipeline(\n",
        "    description=\"pipeline_with_uploaddata\",\n",
        "    workspace=ws,    \n",
        "    steps=[upload_data_step])\n",
        "data_pipeline_run = experiment.submit(data_pipeline, pipeline_parameters={\"ds_name\":dataset})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_pipeline_run.wait_for_completion(show_output=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Pipeline\n",
        "### Prepare Training Data Step\n",
        "\n",
        "Script to check if new data is available since the model was last trained. If no new data is available, we cancel the remaining pipeline steps. We need to set allow_reuse flag to False to allow the pipeline to run even when inputs don't change. We also need the name of the model to check the time the model was last trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.pipeline.core import PipelineData\n",
        "\n",
        "# The model name with which to register the trained model in the workspace.\n",
        "model_name = PipelineParameter(\"model_name\", default_value=\"noaaweatherds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_prep_step = PythonScriptStep(script_name=\"check_data.py\", \n",
        "                                         allow_reuse=False,\n",
        "                                         name=\"check_data\",\n",
        "                                         arguments=[\"--ds_name\", ds_name,\n",
        "                                                    \"--model_name\", model_name],\n",
        "                                         compute_target=compute_target, \n",
        "                                         runconfig=conda_run_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core import Dataset\n",
        "train_ds = Dataset.get_by_name(ws, dataset)\n",
        "train_ds = train_ds.drop_columns([\"partition_date\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AutoMLStep\n",
        "Create an AutoMLConfig and a training step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.train.automl import AutoMLConfig\n",
        "from azureml.pipeline.steps import AutoMLStep\n",
        "\n",
        "automl_settings = {\n",
        "    \"iteration_timeout_minutes\": 10,\n",
        "    \"experiment_timeout_hours\": 0.25,\n",
        "    \"n_cross_validations\": 3,\n",
        "    \"primary_metric\": 'r2_score',\n",
        "    \"max_concurrent_iterations\": 3,\n",
        "    \"max_cores_per_iteration\": -1,\n",
        "    \"verbosity\": logging.INFO,\n",
        "    \"enable_early_stopping\": True\n",
        "}\n",
        "\n",
        "automl_config = AutoMLConfig(task = 'regression',\n",
        "                             debug_log = 'automl_errors.log',\n",
        "                             path = \".\",\n",
        "                             compute_target=compute_target,\n",
        "                             training_data = train_ds,\n",
        "                             label_column_name = target_column_name,\n",
        "                             **automl_settings\n",
        "                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.pipeline.core import PipelineData, TrainingOutput\n",
        "\n",
        "metrics_output_name = 'metrics_output'\n",
        "best_model_output_name = 'best_model_output'\n",
        "\n",
        "metrics_data = PipelineData(name='metrics_data',\n",
        "                           datastore=dstor,\n",
        "                           pipeline_output_name=metrics_output_name,\n",
        "                           training_output=TrainingOutput(type='Metrics'))\n",
        "model_data = PipelineData(name='model_data',\n",
        "                           datastore=dstor,\n",
        "                           pipeline_output_name=best_model_output_name,\n",
        "                           training_output=TrainingOutput(type='Model'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "automl_step = AutoMLStep(\n",
        "    name='automl_module',\n",
        "    automl_config=automl_config,\n",
        "    outputs=[metrics_data, model_data],\n",
        "    allow_reuse=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Register Model Step\n",
        "Script to register the model to the workspace. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "register_model_step = PythonScriptStep(script_name=\"register_model.py\",\n",
        "                                       name=\"register_model\",\n",
        "                                       allow_reuse=False,\n",
        "                                       arguments=[\"--model_name\", model_name, \"--model_path\", model_data, \"--ds_name\", ds_name],\n",
        "                                       inputs=[model_data],\n",
        "                                       compute_target=compute_target,\n",
        "                                       runconfig=conda_run_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Submit Pipeline Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_pipeline = Pipeline(\n",
        "    description=\"training_pipeline\",\n",
        "    workspace=ws,    \n",
        "    steps=[data_prep_step, automl_step, register_model_step])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_pipeline_run = experiment.submit(training_pipeline, pipeline_parameters={\n",
        "        \"ds_name\": dataset, \"model_name\": \"noaaweatherds\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_pipeline_run.wait_for_completion(show_output=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Publish Retraining Pipeline and Schedule\n",
        "Once we are happy with the pipeline, we can publish the training pipeline to the workspace and create a schedule to trigger on blob change. The schedule polls the blob store where the data is being uploaded and runs the retraining pipeline if there is a data change. A new version of the model will be registered to the workspace once the run is complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_name = \"Retraining-Pipeline-NOAAWeather\"\n",
        "\n",
        "published_pipeline = training_pipeline.publish(\n",
        "    name=pipeline_name, \n",
        "    description=\"Pipeline that retrains AutoML model\")\n",
        "\n",
        "published_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.pipeline.core import Schedule\n",
        "schedule = Schedule.create(workspace=ws, name=\"RetrainingSchedule\",\n",
        "                           pipeline_parameters={\"ds_name\": dataset, \"model_name\": \"noaaweatherds\"},\n",
        "                           pipeline_id=published_pipeline.id, \n",
        "                           experiment_name=experiment_name, \n",
        "                           datastore=dstor,\n",
        "                           wait_for_provisioning=True,\n",
        "                           polling_interval=1440)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Retraining\n",
        "Here we setup the data ingestion pipeline to run on a schedule, to verify that the retraining pipeline runs as expected. \n",
        "\n",
        "Note: \n",
        "* Azure NOAA Weather data is updated daily and retraining will not trigger if there is no new data available. \n",
        "* Depending on the polling interval set in the schedule, the retraining may take some time trigger after data ingestion pipeline completes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_name = \"DataIngestion-Pipeline-NOAAWeather\"\n",
        "\n",
        "published_pipeline = training_pipeline.publish(\n",
        "    name=pipeline_name, \n",
        "    description=\"Pipeline that updates NOAAWeather Dataset\")\n",
        "\n",
        "published_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.pipeline.core import Schedule\n",
        "schedule = Schedule.create(workspace=ws, name=\"RetrainingSchedule-DataIngestion\",\n",
        "                           pipeline_parameters={\"ds_name\":dataset},\n",
        "                           pipeline_id=published_pipeline.id, \n",
        "                           experiment_name=experiment_name, \n",
        "                           datastore=dstor,\n",
        "                           wait_for_provisioning=True,\n",
        "                           polling_interval=1440)"
      ]
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "vivijay"
      }
    ],
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}