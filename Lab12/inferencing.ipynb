{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Real-Time Inferencing Service\n",
    "\n",
    "After training a predictive model, you can deploy it as a real-time service that clients can use to get predictions from new data.\n",
    "\n",
    "I've adapted the code found in our documentation for [creating a real-time inferening service](https://github.com/MicrosoftDocs/mslearn-aml-labs/blob/master/06-Deploying_a_model.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use Azure ML 1.27.0 to work with mlops\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core import Model\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the folder for the experiment files used\n",
    "training_folder = 'driver-training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driver_model version: 3\n",
      "\t Training context : Pipeline\n",
      "\n",
      "\n",
      "driver_model.pkl version: 4\n",
      "\n",
      "\n",
      "driver_model.pkl version: 3\n",
      "\n",
      "\n",
      "driver_model version: 2\n",
      "\t Training context : Pipeline\n",
      "\n",
      "\n",
      "driver_model.pkl version: 2\n",
      "\n",
      "\n",
      "driver_model version: 1\n",
      "\t Training context : Pipeline\n",
      "\n",
      "\n",
      "driver_model.pkl version: 1\n",
      "\n",
      "\n",
      "compliance-classifier version: 17\n",
      "\t type : classification\n",
      "\t run_id : 0767a613-e8c2-4811-ae6b-64369e340725\n",
      "\t build_number : 20201217.1\n",
      "\n",
      "\n",
      "BikeBuyer.mml version: 4\n",
      "\n",
      "\n",
      "AutoMLb9be0a22f28 version: 1\n",
      "\n",
      "\n",
      "compliance-classifier version: 16\n",
      "\t type : classification\n",
      "\t run_id : a7eb32c9-3207-4a2c-865a-d65fba7946ba\n",
      "\t build_number : 20201119.1\n",
      "\n",
      "\n",
      "compliance-classifier version: 15\n",
      "\t type : classification\n",
      "\t run_id : 8e84d5f5-54c9-48c9-b733-9bbbcd86558b\n",
      "\t build_number : 20201118.1\n",
      "\n",
      "\n",
      "IBM_attrition_model version: 1\n",
      "\t area : HR\n",
      "\t type : attrition\n",
      "\n",
      "\n",
      "AutoML015fd913221 version: 1\n",
      "\n",
      "\n",
      "diabetes_model version: 1\n",
      "\t Training context : Inline Training\n",
      "\t AUC : 0.8857431111811085\n",
      "\t Accuracy : 0.9002222222222223\n",
      "\n",
      "\n",
      "glove-text-classifier version: 1\n",
      "\t type : classification\n",
      "\t run_id : glove-embeddings-classifier_1597854176_900c5cea\n",
      "\n",
      "\n",
      "compliance-classifier version: 14\n",
      "\t type : classification\n",
      "\t run_id : 3886cc4f-8a9d-4608-abea-c588a9074912\n",
      "\t build_number : 20200428.2\n",
      "\n",
      "\n",
      "compliance-classifier version: 13\n",
      "\t type : classification\n",
      "\t run_id : 36b88b87-4c5e-4d64-8fa3-ae721eda608d\n",
      "\t build_number : 20200428.1\n",
      "\n",
      "\n",
      "compliance-classifier version: 12\n",
      "\t type : classification\n",
      "\t run_id : 46b16ce1-7719-4faa-84ae-6d6a4835da08\n",
      "\t build_number : 20200427.1\n",
      "\n",
      "\n",
      "compliance-classifier-qvc version: 1\n",
      "\t type : classification\n",
      "\t run_id : deep-learning-qvc_1587989276_00ba8da2\n",
      "\n",
      "\n",
      "BikeBuyer.mml version: 3\n",
      "\n",
      "\n",
      "compliance-classifier version: 11\n",
      "\t type : classification\n",
      "\t run_id : 8c546c17-d0a9-43dc-92c6-c464fdfe68e2\n",
      "\t build_number : 20200109.1\n",
      "\n",
      "\n",
      "compliance-classifier version: 10\n",
      "\t type : classification\n",
      "\t run_id : 3d1df6b1-fd5e-42e1-9021-44038a2aeabd\n",
      "\t build_number : 20200109.1\n",
      "\n",
      "\n",
      "compliance-classifier version: 9\n",
      "\t type : classification\n",
      "\t run_id : 22e820b2-1015-4ac2-88d4-c80eed9e0006\n",
      "\t build_number : 20200109.1\n",
      "\n",
      "\n",
      "compliance-classifier version: 8\n",
      "\t type : classification\n",
      "\t run_id : 1d751a7d-5470-473a-8e9b-5b22be5fb2bb\n",
      "\t build_number : 20191028.1\n",
      "\n",
      "\n",
      "compliance-classifier version: 7\n",
      "\t type : classification\n",
      "\t run_id : db72b465-9ae9-4e22-8a3c-22b81e09478b\n",
      "\t build_number : 20191025.1\n",
      "\n",
      "\n",
      "compliance-classifier version: 6\n",
      "\t type : classification\n",
      "\t run_id : 5c59dcd9-f23b-4abf-a1b5-af151927f0e7\n",
      "\t build_number : 20191024.4\n",
      "\n",
      "\n",
      "compliance-classifier version: 5\n",
      "\t type : classification\n",
      "\t run_id : 5c7a146e-bb1a-40a8-a3a0-ac1f953205f4\n",
      "\t build_number : 20191024.3\n",
      "\n",
      "\n",
      "compliance-classifier version: 4\n",
      "\t type : classification\n",
      "\t run_id : 1a7ba8f6-f615-4ec2-8c1f-6926c2517234\n",
      "\t build_number : 20191024.2\n",
      "\n",
      "\n",
      "compliance-classifier version: 3\n",
      "\t type : classification\n",
      "\t run_id : 138fdae4-b92f-494d-8d63-ed884f655332\n",
      "\t build_number : 20191024.2\n",
      "\n",
      "\n",
      "compliance-classifier version: 2\n",
      "\n",
      "\n",
      "compliance-classifier version: 1\n",
      "\t type : classification\n",
      "\t run_id : deep-learning_1571935012_a8c321f6\n",
      "\n",
      "\n",
      "BikeBuyer.mml version: 2\n",
      "\n",
      "\n",
      "BikeBuyer.mml version: 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List the models registered in the workspace\n",
    "\n",
    "for model in Model.list(ws):\n",
    "    print(model.name, 'version:', model.version)\n",
    "    for tag_name in model.tags:\n",
    "        tag = model.tags[tag_name]\n",
    "        print ('\\t',tag_name, ':', tag)\n",
    "    for prop_name in model.properties:\n",
    "        prop = model.properties[prop_name]\n",
    "        print ('\\t',prop_name, ':', prop)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driver_model version 3\n"
     ]
    }
   ],
   "source": [
    "# Get the latest registered version of the model\n",
    "model = ws.models['driver_model']\n",
    "print(model.name, 'version', model.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/davew202105/code/git/MLOps-E2E-sdkv2/Lab12\n"
     ]
    }
   ],
   "source": [
    "# you need to figure out where you want to put the service folder\n",
    "# ideally this should be at the same level as the driver-training folder\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf-svc folder created.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# changeme\n",
    "service_path = 'inf-svc'\n",
    "\n",
    "# Create a folder for the web service files\n",
    "os.makedirs(service_path, exist_ok=True)\n",
    "\n",
    "print(service_path, 'folder created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The web service where we deploy the model will need some Python code to load the input data, get the model from the workspace, and generate and return predictions. We'll save this code in an entry script (often called a scoring script: the convention `score.py`) that will be deployed to the web service:\n",
    "\n",
    "Steps: \n",
    "\n",
    "* Open the score.py file in the `Lab12` folder.  \n",
    "* Get a feel for what it is doing\n",
    "* you might need to change the model name\n",
    "* this is what our inferencing webservice will call\n",
    "* we need to upload this file into our `service_path` folder.  do that now (you can just copy/paste from Lab12 to `service_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dependency info in inf-svc/driver_env.yml\n",
      "# Conda environment specification. The dependencies defined in this file will\n",
      "# be automatically provisioned for runs with userManagedDependencies=False.\n",
      "\n",
      "# Details about the Conda environment file format:\n",
      "# https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually\n",
      "\n",
      "name: project_environment\n",
      "dependencies:\n",
      "  # The python interpreter version.\n",
      "  # Currently Azure ML only supports 3.5.2 and later.\n",
      "- python=3.6.2\n",
      "\n",
      "- pip:\n",
      "    # Required packages for AzureML execution, history, and data preparation.\n",
      "  - azureml-defaults\n",
      "\n",
      "  - lightgbm\n",
      "- scikit-learn\n",
      "- pandas\n",
      "channels:\n",
      "- anaconda\n",
      "- conda-forge\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now we need to add our model dependencies (AzureML defaults is already included)\n",
    "# this is similar to what we did in previous labs but now we are going to save the environment file to a yaml file\n",
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "# we can get these from our training script\n",
    "myenv = CondaDependencies()\n",
    "myenv.add_conda_package(\"scikit-learn\")\n",
    "myenv.add_conda_package(\"pandas\")\n",
    "myenv.add_pip_package(\"lightgbm\")\n",
    "\n",
    "# Save the environment config as a .yml file\n",
    "env_file = service_path + \"/driver_env.yml\"\n",
    "with open(env_file,\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())\n",
    "print(\"Saved dependency info in\", env_file)\n",
    "\n",
    "# Print the .yml file\n",
    "with open(env_file,\"r\") as f:\n",
    "    print(f.read())\n",
    "\n",
    "# the yml file and the score.py should likely be put under source control correctly in the \n",
    "# driver-service folder.  \n",
    "# examine the created driver_env.yml so you understand it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driver_env.yml\tscore.py\n"
     ]
    }
   ],
   "source": [
    "# we should have score.py and driver_env.yml in this folder\n",
    "# we will use those next\n",
    "!ls $service_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deployment process includes the following steps:\n",
    "\n",
    "1. Define an inference configuration, which includes the scoring and environment files required to load and use the model.\n",
    "2. Define a deployment configuration that defines the execution environment in which the service will be hosted. In this case, an Azure Container Instance.\n",
    "3. Deploy the model as a web service.\n",
    "4. Verify the status of the deployed service.\n",
    "\n",
    "> **More Information**: For more details about model deployment, and options for target execution environments, see the [documentation](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-and-where).\n",
    "\n",
    "Deployment will take some time as it first runs a process to create a container image, and then runs a process to create a web service based on the image. When deployment has completed successfully, you'll see a status of **Healthy**.\n",
    "\n",
    "## Scoring/Inferencing Configuration\n",
    "\n",
    "* Now we need to configure the scoring environment.  \n",
    "* We'll use an ACI environment for this, but AKS is another approach that works well but requires a few more steps. We'll do that in another lab later.  \n",
    "* You can view the status of the deployment in the AMLS portal under `Endpoints`.  \n",
    "* the first deployment always takes a little while.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2021-06-01 14:20:09+00:00 Creating Container Registry if not exists.\n",
      "2021-06-01 14:20:09+00:00 Registering the environment.\n",
      "2021-06-01 14:20:12+00:00 Use the existing image.\n",
      "2021-06-01 14:20:12+00:00 Generating deployment configuration.\n",
      "2021-06-01 14:20:13+00:00 Submitting deployment to compute..\n",
      "2021-06-01 14:20:41+00:00 Checking the status of deployment driver-service-sdk2..\n",
      "2021-06-01 14:22:28+00:00 Checking the status of inference endpoint driver-service-sdk2.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Healthy\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "# Configure the scoring environment\n",
    "inference_config = InferenceConfig(runtime= \"python\",\n",
    "                                   source_directory = service_path,\n",
    "                                   entry_script=\"score.py\",\n",
    "                                   conda_file=\"driver_env.yml\")\n",
    "\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)\n",
    "\n",
    "service_name = \"driver-service-sdk2\"\n",
    "\n",
    "service = Model.deploy(ws, service_name, [model], inference_config, deployment_config)\n",
    "\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, the deployment has been successful and you can see a status of **Healthy**. If not, you can use the following code to check the status and get the service logs to help you troubleshoot.\n",
    "\n",
    "Take a look at your workspace in [Azure ML Studio](https://ml.azure.com) and view the **Endpoints** page, which shows the deployed services in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you need to make a change and redeploy you may need to delete the old, unhealthy service.  Use the following code:\n",
    "# service.delete()\n",
    "# but don't delete the service yet after you get a good deployment.  We will use test the service next.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driver-service-sdk2\n",
      "driver-service\n",
      "bikebuyeraciws\n",
      "ar-factoring-2class\n",
      "predict-attrition-svc1\n",
      "support-ticket-duration\n",
      "bikebuyer2-aks-service\n",
      "bikebuyer-aci\n",
      "compliance-classifier-service\n",
      "bb-aks-service\n"
     ]
    }
   ],
   "source": [
    "# print the webservice endpoints\n",
    "for webservice_name in ws.webservices:\n",
    "    print(webservice_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-01T14:22:15,046221600+00:00 - iot-server/run \n",
      "2021-06-01T14:22:15,047277400+00:00 - rsyslog/run \n",
      "2021-06-01T14:22:15,047277300+00:00 - gunicorn/run \n",
      "2021-06-01T14:22:15,113525700+00:00 - nginx/run \n",
      "/usr/sbin/nginx: /azureml-envs/azureml_deade6e796a908fdec1522cb257b3da0/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_deade6e796a908fdec1522cb257b3da0/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_deade6e796a908fdec1522cb257b3da0/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_deade6e796a908fdec1522cb257b3da0/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_deade6e796a908fdec1522cb257b3da0/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "2021-06-01T14:22:15,452906500+00:00 - iot-server/finish 1 0\n",
      "2021-06-01T14:22:15,459122500+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "Starting gunicorn 20.1.0\n",
      "Listening at: http://127.0.0.1:31311 (68)\n",
      "Using worker: sync\n",
      "worker timeout is set to 300\n",
      "Booting worker with pid: 97\n",
      "SPARK_HOME not set. Skipping PySpark Initialization.\n",
      "Initializing logger\n",
      "2021-06-01 14:22:17,898 | root | INFO | Starting up app insights client\n",
      "2021-06-01 14:22:17,898 | root | INFO | Starting up request id generator\n",
      "2021-06-01 14:22:17,899 | root | INFO | Starting up app insight hooks\n",
      "2021-06-01 14:22:17,899 | root | INFO | Invoking user's init function\n",
      "2021-06-01 14:22:19,126 | root | INFO | Users's init has completed successfully\n",
      "2021-06-01 14:22:19,130 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\n",
      "2021-06-01 14:22:19,131 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\n",
      "2021-06-01 14:22:19,135 | root | INFO | Scoring timeout is found from os.environ: 60000 ms\n",
      "2021-06-01 14:22:28,967 | root | INFO | Swagger file not present\n",
      "2021-06-01 14:22:28,967 | root | INFO | 404\n",
      "127.0.0.1 - - [01/Jun/2021:14:22:28 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "2021-06-01 14:22:32,235 | root | INFO | Swagger file not present\n",
      "2021-06-01 14:22:32,236 | root | INFO | 404\n",
      "127.0.0.1 - - [01/Jun/2021:14:22:32 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "2021-06-01 14:22:47,674 | root | INFO | Swagger file not present\n",
      "2021-06-01 14:22:47,674 | root | INFO | 404\n",
      "127.0.0.1 - - [01/Jun/2021:14:22:47 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# these are helpful commands\n",
    "#print(service.state)\n",
    "print(service.get_logs())\n",
    "#service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Web Service\n",
    "\n",
    "With the service deployed, now you can consume it from a client application.  Let's simulate that with python, but using the SDK initially.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:  {'result': [0.027311045841034335, 0.0261231327307869]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# this is our test data, it's actually 2 observations\n",
    "TEST_ROW = [[0,1,8,1,0,0,1,0,0,0,0,0,0,0,12,1,0,0,0.5,0.3,0.610327781,7,1,-1,0,-1,1,1,1,2,1,65,1,0.316227766,0.669556409,0.352136337,3.464101615,0.1,0.8,0.6,1,1,6,3,6,2,9,1,1,1,12,0,1,1,0,0,1],\n",
    "            [4,2,5,1,0,0,0,0,1,0,0,0,0,0,5,1,0,0,0.9,0.5,0.771362431,4,1,-1,0,0,11,1,1,0,1,103,1,0.316227766,0.60632002,0.358329457,2.828427125,0.4,0.5,0.4,3,3,8,4,10,2,7,2,0,3,10,0,0,1,1,0,1]\n",
    "           ]\n",
    "# convert the array to a serializable list in a JSON document\n",
    "input_json = json.dumps({\"data\": TEST_ROW})\n",
    "\n",
    "# call the web service, passing the input data (it will aslo accept the data in binary format)\n",
    "predictions = service.run (input_data = input_json)\n",
    "print(\"predictions: \", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above uses the Azure ML SDK to connect to the containerized web service and use it to generate predictions from your model. In production, a model is likely to be consumed by business applications that do not use the Azure ML SDK, but simply make HTTP requests to the web service.\n",
    "\n",
    "Let's determine the URL to which these applications must submit their requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://07f0d01a-35fc-4398-9234-c2e936c593e9.eastus.azurecontainer.io/score\n"
     ]
    }
   ],
   "source": [
    "endpoint = service.scoring_uri\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know the endpoint URI, an application can simply make an HTTP request, sending the input data in JSON (or binary) format, and receive back the predicted class(es)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{\"result\": [0.027311045841034335, 0.0261231327307869]}\n"
     ]
    }
   ],
   "source": [
    "# we'll recycle the same inputs and vars as above, but add some scaffolding to simulate an API call\n",
    "import requests\n",
    "\n",
    "# Set the content type\n",
    "headers = { 'Content-Type':'application/json' }\n",
    "\n",
    "predictions = requests.post(endpoint, input_json, headers = headers)\n",
    "print (predictions)\n",
    "print (predictions.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've deployed your web service as an Azure Container Instance (ACI) service that requires no authentication. This is fine for development and testing, but for production you should consider deploying to an Azure Kubernetes Service (AKS) cluster and enabling authentication. This would require REST requests to include an **Authorization** header.\n",
    "\n",
    "## Delete the Service\n",
    "\n",
    "When you no longer need your service, you should delete it to avoid incurring unecessary charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()\n",
    "print ('Service deleted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about publishing a model as a service, see the [documentation](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-and-where)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
